{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1uSOkikafN/AWuCdvdpaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j0hndufek/Brain-Tumor-MRI-CNN/blob/main/BrainTumorMRI_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lKggQocxo_28"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc(\"font\", size=14)\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "#from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD"
      ],
      "metadata": {
        "id": "YZ02_NnzpH76"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "C9wJJWf8pKg6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVrOIdC_pMoZ",
        "outputId": "bd46ed43-cd98-4c93-e063-1bd9160063b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img = []\n",
        "train_labels = []\n",
        "\n",
        "test_img = []\n",
        "test_labels = []\n",
        "\n",
        "#path_train = ('/Users/johndufek/Downloads/archive (2)/Training/')# code for local directory (e.g. in jupyter)\n",
        "#path_test = ('/Users/johndufek/Downloads/archive (2)/Testing/')\n",
        "path_train = ('/content/gdrive/MyDrive/archive (2)/Training/')\n",
        "path_test = ('/content/gdrive/MyDrive/archive (2)/Testing/')\n",
        "\n",
        "img_size= 300\n",
        "\n",
        "for i in os.listdir(path_train):\n",
        "    for j in os.listdir(path_train+i):\n",
        "        train_img.append (cv2.resize(cv2.imread(path_train+i+'/'+j), (img_size,img_size)))\n",
        "        train_labels.append(i)\n",
        "\n",
        "for i in os.listdir(path_test):\n",
        "    for j in os.listdir(path_test+i):\n",
        "        test_img.append (cv2.resize(cv2.imread(path_test+i+'/'+j), (img_size,img_size)))\n",
        "        test_labels.append(i)\n",
        "\n",
        "train_img = (np.array(train_img))\n",
        "test_img = (np.array(test_img))\n",
        "\n",
        "\n",
        "train_labels_encoded = [0 if category == 'no_tumor' else(1 if category == 'glioma_tumor' else(2 if category=='meningioma_tumor' else 3)) for category in list(train_labels)]\n",
        "test_labels_encoded = [0 if category == 'no_tumor' else(1 if category == 'glioma_tumor' else(2 if category=='meningioma_tumor' else 3)) for category in list(test_labels)]\n",
        "\n"
      ],
      "metadata": {
        "id": "wffuhBdypOmW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform function to normalize photo data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],\n",
        "                        [0.5,0.5,0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "ZpxUhIK8pRVL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Training and Validation data sets\n",
        "train_x, val_x, train_y, val_y = train_test_split(np.array(train_img), np.array(train_labels), test_size = 0.20)"
      ],
      "metadata": {
        "id": "10wthU-zphoZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_load=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(path_train,transform=transform),\n",
        "    batch_size=64, shuffle=True\n",
        ")\n",
        "test_load=DataLoader(\n",
        "    torchvision.datasets.ImageFolder(path_test,transform=transform),\n",
        "    batch_size=32, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "jcOQC7Jmpkq1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_load)\n",
        "images,labels = next(dataiter)"
      ],
      "metadata": {
        "id": "a-6eTBDJrfHn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "pool = nn.MaxPool2d(2, 2)"
      ],
      "metadata": {
        "id": "Yoms5i_IrgBH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = conv1(images)\n",
        "x = pool(x)\n",
        "x = conv2(x)\n",
        "x = pool(x)\n",
        "x = conv3(x)\n",
        "x = pool(x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WBwSOJhr6cO",
        "outputId": "afbdb699-ef36-4595-9e6a-fe36cd913f4d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 128, 18, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128*18*18, 512)\n",
        "        self.fc2 = nn.Linear(512, 4)  # Output classes\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 128*18*18)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Configure Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 30\n",
        "lrn_rate = 0.001\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = CNN().to(device)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "opt = Adam(model.parameters(), lr=lrn_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_load:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_f(outputs, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_load)\n",
        "    train_accuracy = 100. * correct / total\n",
        "\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_load:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100. * test_correct / test_total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}] | '\n",
        "          f'Train Loss: {train_loss:.4f} | '\n",
        "          f'Train Acc: {train_accuracy:.2f}% | '\n",
        "          f'Test Acc: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9VNsLiVplYB",
        "outputId": "590da581-0b14-4634-d9ad-cfcb25008ef9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30] | Train Loss: 3.7679 | Train Acc: 52.94% | Test Acc: 30.46%\n",
            "Epoch [2/30] | Train Loss: 0.7199 | Train Acc: 68.84% | Test Acc: 36.04%\n",
            "Epoch [3/30] | Train Loss: 0.6770 | Train Acc: 69.82% | Test Acc: 44.42%\n",
            "Epoch [4/30] | Train Loss: 0.5576 | Train Acc: 76.55% | Test Acc: 50.00%\n",
            "Epoch [5/30] | Train Loss: 0.5030 | Train Acc: 79.26% | Test Acc: 54.57%\n",
            "Epoch [6/30] | Train Loss: 0.4685 | Train Acc: 80.62% | Test Acc: 51.52%\n",
            "Epoch [7/30] | Train Loss: 0.4174 | Train Acc: 83.40% | Test Acc: 57.11%\n",
            "Epoch [8/30] | Train Loss: 0.3799 | Train Acc: 84.13% | Test Acc: 58.12%\n",
            "Epoch [9/30] | Train Loss: 0.3261 | Train Acc: 87.53% | Test Acc: 59.39%\n",
            "Epoch [10/30] | Train Loss: 0.2881 | Train Acc: 88.68% | Test Acc: 61.42%\n",
            "Epoch [11/30] | Train Loss: 0.2679 | Train Acc: 89.51% | Test Acc: 64.47%\n",
            "Epoch [12/30] | Train Loss: 0.2187 | Train Acc: 91.91% | Test Acc: 63.45%\n",
            "Epoch [13/30] | Train Loss: 0.2108 | Train Acc: 91.18% | Test Acc: 65.99%\n",
            "Epoch [14/30] | Train Loss: 0.1952 | Train Acc: 92.36% | Test Acc: 63.45%\n",
            "Epoch [15/30] | Train Loss: 0.1500 | Train Acc: 94.55% | Test Acc: 69.04%\n",
            "Epoch [16/30] | Train Loss: 0.1835 | Train Acc: 93.50% | Test Acc: 66.50%\n",
            "Epoch [17/30] | Train Loss: 0.1391 | Train Acc: 94.89% | Test Acc: 66.50%\n",
            "Epoch [18/30] | Train Loss: 0.1467 | Train Acc: 93.92% | Test Acc: 71.32%\n",
            "Epoch [19/30] | Train Loss: 0.1180 | Train Acc: 95.94% | Test Acc: 69.54%\n",
            "Epoch [20/30] | Train Loss: 0.1161 | Train Acc: 95.42% | Test Acc: 71.83%\n",
            "Epoch [21/30] | Train Loss: 0.1018 | Train Acc: 96.25% | Test Acc: 72.08%\n",
            "Epoch [22/30] | Train Loss: 0.0806 | Train Acc: 97.01% | Test Acc: 72.08%\n",
            "Epoch [23/30] | Train Loss: 0.0731 | Train Acc: 97.50% | Test Acc: 70.30%\n",
            "Epoch [24/30] | Train Loss: 0.0693 | Train Acc: 97.46% | Test Acc: 71.32%\n",
            "Epoch [25/30] | Train Loss: 0.0601 | Train Acc: 97.99% | Test Acc: 72.84%\n",
            "Epoch [26/30] | Train Loss: 0.0587 | Train Acc: 97.99% | Test Acc: 72.34%\n",
            "Epoch [27/30] | Train Loss: 0.0641 | Train Acc: 97.60% | Test Acc: 72.59%\n",
            "Epoch [28/30] | Train Loss: 0.0398 | Train Acc: 98.71% | Test Acc: 73.35%\n",
            "Epoch [29/30] | Train Loss: 0.0388 | Train Acc: 98.85% | Test Acc: 74.11%\n",
            "Epoch [30/30] | Train Loss: 0.0329 | Train Acc: 99.03% | Test Acc: 71.32%\n"
          ]
        }
      ]
    }
  ]
}